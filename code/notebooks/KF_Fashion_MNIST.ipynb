{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azKzys_66G42"
   },
   "source": [
    "# From Notebook to clothing images classification pipeline using Fashion MNIST\n",
    "\n",
    "This notebook is based on the github project [From Notebook to Kubeflow Pipeline using Fashion MNIST](https://github.com/manceps/fashion-mnist-kfp-lab/blob/master/KF_Fashion_MNIST.ipynb) under the [MIT License](https://github.com/manceps/fashion-mnist-kfp-lab/blob/master/LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azKzys_66G42"
   },
   "source": [
    "# From notebook to Kubeflow Pipeline using Fahion MNIST\n",
    "\n",
    "In this notebook, we will walk you through the steps of converting a machine learning model, which you may already have on a jupyter notebook, into a Kubeflow pipeline. As an example, we will make use of the fashion we will make use of the fashion MNIST dataset and the [Basic classification with Tensorflow](https://www.tensorflow.org/tutorials/keras/classification) example.\n",
    "\n",
    "In this example we use:\n",
    "\n",
    "* **Kubeflow pipelines** - [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) is a machine learning workflow platform that is helping data scientists and ML engineers tackle experimentation and productionization of ML workloads. It allows users to easily orchestrate scalable workloads using an SDK right from the comfort of a Jupyter Notebook.\n",
    "\n",
    "**Note:** This notebook is to be run on a notebook server inside the Kubeflow environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kN5XA9ybEtwU"
   },
   "source": [
    "## Section 1: Data exploration (as in [here](https://www.tensorflow.org/tutorials/keras/classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ4EgzcMEUko"
   },
   "source": [
    "The [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)  dataset contains 70,000 grayscale images in 10 clothing categories. Each image is 28x28 pixels in size. We chose this dataset to demonstrate the funtionality of Kubeflow Pipelines without introducing too much complexity in the implementation of the ML model.\n",
    "\n",
    "To familiarize you with the dataset we will do a short exploration. It is always a good idea to understand your data before you begin any kind of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMIxLNEiGR3x"
   },
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user --upgrade pandas matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y4DB_1u5H1fT",
    "outputId": "58a8d65c-e978-4af2-aa85-114d4a7f3f29"
   },
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data exploration libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OW5-_PC3H3YB"
   },
   "source": [
    "### 1.2 Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrDlsMh4LoXz"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9FDsUlxCaWW"
   },
   "source": [
    "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vec35x3cMQfo"
   },
   "outputs": [],
   "source": [
    "class_names = ['Top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Y_88GQbMX1G"
   },
   "source": [
    "Let's look at the format of each dataset split. The training set contains 60,000 images and the test set contains 10,000 images which are each 28x28 pixels.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "e2-Zbu0BMpt1",
    "outputId": "51310a2d-ad13-4127-f1eb-db3237524044"
   },
   "outputs": [],
   "source": [
    "print(f'Number of training images: {train_images.shape[0]}\\n')\n",
    "print(f'Number of test images: {test_images.shape[0]}\\n')\n",
    "\n",
    "print(f'Image size: {train_images.shape[1:]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHF6g2dQNQVc"
   },
   "source": [
    "There are logically 60,000 training labels and 10,000 test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "GUsWcVaUNVev",
    "outputId": "c13c5423-b779-4b3d-e0a7-54b03b8fc067"
   },
   "outputs": [],
   "source": [
    "print(f'Number of labels: {len(train_labels)}\\n')\n",
    "print(f'Number of test labels: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSk_ogerNlq4"
   },
   "source": [
    "Each label is an integer between 0 and 9 corresponding to the 10 class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "jkDXYdJ5N5zk",
    "outputId": "55b5a3c4-aae3-46b0-a5b0-a4a31e0c610e"
   },
   "outputs": [],
   "source": [
    "unique_train_labels = np.unique(train_labels)\n",
    "\n",
    "for label in zip(class_names, unique_train_labels):\n",
    "    label_name, label_num = label\n",
    "    print(f'{label_name}: {label_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVOPKTgdP-nS"
   },
   "source": [
    "To properly train the model, the data must be normalized so each value will fall between 0 and 1. Later on this step will be done inside of the training script but we will show what that process looks like here.\n",
    "\n",
    "The first image shows that the values fall in a range between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "m4VEw8Ud9Quh",
    "outputId": "9942926b-6ee4-4f84-e427-56e67a86d6f8"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn88kMUcR5G5"
   },
   "source": [
    "To scale the data we divide the training and test values by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VREJVpLRqeP"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ocp5nOyjSBEe"
   },
   "source": [
    "We plot the first 25 images from the training set to show that the data is in fact in the form we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "-KzmYh0kSLSG",
    "outputId": "8d7c3525-39a0-40c4-a5a7-e63593f9fc88"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igI1dvp7SWWV"
   },
   "source": [
    "# Section 2: Kubeflow pipeline building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, all our steps are similar to what you can find in the [Basic classification with Tensorflow](https://https://www.tensorflow.org/tutorials/keras/classification) example. The next step given in [From Notebook to Kubeflow Pipeline using Fashion MNIST](https://github.com/manceps/fashion-mnist-kfp-lab/blob/master/KF_Fashion_MNIST.ipynb) is to build the containerized approach provided by Kubeflow to allow our model to be run using Kubernetes. We will be adapting the provided code in order to generate some metrics and change some pipeline components connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install Kubeflow pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ctN1VrL_qnA"
   },
   "source": [
    " The first step is to install the Kubeflow Pipelines SDK package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kWutHs306En6",
    "outputId": "f87b05b5-0555-43ba-8dae-04056821fe1b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBwaov51AFex"
   },
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "iUJZqAuN6EoK",
    "outputId": "8d520295-afdd-456f-a6f4-d9a17fda8ce2"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMWQNog7AZFP"
   },
   "source": [
    "Check if the install was successful:\n",
    "\n",
    "**/usr/local/bin/dsl-compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmHw4UGN6EoX"
   },
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see **/usr/local/bin/dsl-compile** above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8hCvyHloOK6"
   },
   "source": [
    "### 2.2 Build Container Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abg3ycFVFhBC"
   },
   "source": [
    "The following cells define functions that will be transformed into lightweight container components. It is recommended to look at the corresponding Fashion MNIST notebook to match what you see here to the original code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.kubeflow.org/docs/images/pipelines-sdk-lightweight.svg\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "  </td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpeW1k1o6Eo5"
   },
   "outputs": [],
   "source": [
    "def train(compile_optimizer, epochs, validation_split, data_path: comp.OutputPath(), lossplot_path: comp.OutputPath(str)) -> typing.NamedTuple('loss_plot', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "\n",
    "    from tensorflow.python import keras\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import base64\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    # Parse pipeline parameters\n",
    "    epochs = int(epochs)\n",
    "    validation_split = float(validation_split)\n",
    "\n",
    "    def save_loss_plot(history, plot_path):\n",
    "        \"\"\"\n",
    "        history: History object from keras. Its History.history attribute is a record of training loss and validation loss values.\n",
    "        plot_path: path where plot image will be saved.\n",
    "        \"\"\"\n",
    "        # Creation of the plot\n",
    "        loss, val_loss = history.history[\"loss\"], history.history[\"val_loss\"]\n",
    "        fig = plt.figure(figsize=(30, 10))\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.cla()\n",
    "        ax.plot(loss)\n",
    "        ax.plot(val_loss)\n",
    "        ax.set_title(\"Model loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "        # Saving plot in specified path\n",
    "        with open(plot_path, \"wb\") as fd:\n",
    "            plt.savefig(fd)\n",
    "\n",
    "    def get_web_app_from_loss_plot(plot_path):\n",
    "        \"\"\"\n",
    "        plot_path: path where plot image is saved.\n",
    "        return: JSON object representing kubeflow output viewer for web-app.\n",
    "        \"\"\"\n",
    "        # Retrieve encoded bytes of the specified image path\n",
    "        with open(plot_path, \"rb\") as fd:\n",
    "            encoded = base64.b64encode(fd.read()).decode('latin1')\n",
    "\n",
    "        web_app_json = {\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': f\"\"\"<img width=\"100%\" src=\"data:image/png;base64,{encoded}\"/>\"\"\"\n",
    "        }\n",
    "        return web_app_json\n",
    "\n",
    "    # Download the dataset and split into training and test data.\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "    (train_images, train_labels), _ = fashion_mnist.load_data()\n",
    "\n",
    "    # Normalize the data so that the values all fall between 0 and 1.\n",
    "    train_images = train_images / 255.0\n",
    "\n",
    "    # Define the model using Keras.\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=compile_optimizer,\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Run a training job with specified number of epochs\n",
    "    history = model.fit(train_images, train_labels, epochs=epochs, validation_split=validation_split)\n",
    "\n",
    "    # Save loss plot\n",
    "    save_loss_plot(history, lossplot_path)\n",
    "\n",
    "    loss_plot = [get_web_app_from_loss_plot(lossplot_path)]\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Save the model to the specified output path\n",
    "    model.save(f'{data_path}/mnist_model.h5')\n",
    "\n",
    "    print(\"============== END TRAINING ==============\")\n",
    "\n",
    "    # Return specified loss_plot\n",
    "    metadata = {\n",
    "        'outputs' : loss_plot\n",
    "    }\n",
    "\n",
    "    loss_plot = namedtuple('loss_plot', ['mlpipeline_ui_metadata'])\n",
    "    return loss_plot(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pxGKEae6Eo_"
   },
   "outputs": [],
   "source": [
    "def test(data_path: comp.InputPath(), results_path: comp.OutputPath(), labels_dir: comp.OutputPath()) -> typing.NamedTuple('Outputs', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from tensorflow.python import keras\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    # Download the dataset and split into training and test data.\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "    _ , (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "    # Load the saved Keras model\n",
    "    model = keras.models.load_model(f'{data_path}/mnist_model.h5')\n",
    "\n",
    "    # Evaluate the model and print the results\n",
    "    _, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    # Define the class names.\n",
    "    class_names = ['Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    # Define a Softmax layer to define outputs as probabilities\n",
    "    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "    # Classify all the images from the test dataset\n",
    "    pred_labels = [0 for k in test_images]\n",
    "    for image_number in range(len(test_images)):\n",
    "        # Grab an image from the test dataset.\n",
    "        img = test_images[image_number]\n",
    "\n",
    "        # Add the image to a batch where it is the only member.\n",
    "        img = (np.expand_dims(img, 0))\n",
    "\n",
    "        # Predict the label of the image.\n",
    "        predictions = probability_model.predict(img)\n",
    "\n",
    "        # Take the prediction with the highest probability\n",
    "        prediction = np.argmax(predictions[0])\n",
    "        pred_labels[image_number] = prediction\n",
    "\n",
    "        # Retrieve the true label of the image from the test labels.\n",
    "        true_label = test_labels[image_number]\n",
    "\n",
    "        class_prediction = class_names[prediction]\n",
    "        confidence = 100 * np.max(predictions)\n",
    "        actual = class_names[true_label]\n",
    "\n",
    "        # Save results\n",
    "        with open(results_path, 'a+') as result:\n",
    "            result.write(\" Image #:\" + str(image_number) + \" | Prediction: {} | Confidence: {:2.0f}% | Actual: {}\\n\".format(class_prediction, confidence, actual))\n",
    "\n",
    "    if not os.path.exists(labels_dir):\n",
    "        os.makedirs(labels_dir)\n",
    "\n",
    "    # Save true labels and predicted labels and class names for confusion matrix\n",
    "    with open(f'{labels_dir}/true_labels.txt', 'w') as ft:\n",
    "        ft.write(str(list(test_labels)))\n",
    "\n",
    "    with open(f'{labels_dir}/pred_labels.txt', 'w') as fp:\n",
    "        fp.write(str(pred_labels))\n",
    "\n",
    "    with open(f'{labels_dir}/class_names.txt', 'w') as fp:\n",
    "        fp.write(str(class_names))\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "            'name': 'accuracy',  # The name of the metric. Visualized as the column name in the runs table.\n",
    "            'numberValue': str(test_acc),  # The value of the metric. Must be a numeric value.\n",
    "            'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "        }]\n",
    "    }\n",
    "    print('Prediction has been saved successfully!')\n",
    "    \n",
    "    return [json.dumps(metrics)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(labels_dir: comp.InputPath()) -> typing.NamedTuple('conf_m_result', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    # Load class names\n",
    "    with open(f'{labels_dir}/class_names.txt', 'r') as fc:\n",
    "        class_names = eval(fc.read())\n",
    "\n",
    "    # Load test labels and predicted labels\n",
    "    with open(f'{labels_dir}/true_labels.txt', 'r') as ft:\n",
    "        test_labels = eval(ft.read())\n",
    "\n",
    "    with open(f'{labels_dir}/pred_labels.txt', 'r') as fp:\n",
    "        pred_labels = eval(fp.read())\n",
    "\n",
    "    # Build confusion matrix\n",
    "    confusion_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "\n",
    "    csv_literal_confusion_matrix = \"\"\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            csv_literal_confusion_matrix += \"{target},{predicted},{count}\\n\".format(target=class_names[i], predicted=class_names[j], count=confusion_matrix[i][j])\n",
    "\n",
    "    kf_literal_confusion_matrix = {\n",
    "        'outputs' : [{\n",
    "            'type': 'confusion_matrix',\n",
    "            'format': 'csv',\n",
    "            'schema': [\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            'storage': 'inline',\n",
    "            'source': csv_literal_confusion_matrix,\n",
    "            'labels': class_names,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    confusion_matrix_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    return confusion_matrix_result(json.dumps(kf_literal_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and predict lightweight components.\n",
    "train_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3', packages_to_install=['matplotlib'])\n",
    "test_op = comp.func_to_container_op(test, base_image='tensorflow/tensorflow:latest-gpu-py3', packages_to_install=['scikit-learn'])\n",
    "confusion_matrix_op = comp.func_to_container_op(confusion_matrix,  packages_to_install=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqwBi1Cro49W"
   },
   "source": [
    "Our next step will be to create the various components that will make up the pipeline. Define the pipeline using the *@dsl.pipeline* decorator.\n",
    "\n",
    "The pipeline function is defined and includes a number of paramters that will be fed into our various components throughout execution. Kubeflow Pipelines are created decalaratively. This means that the code is not run until the pipeline is compiled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "Langpu8q6Ept",
    "outputId": "30769082-0be7-4db4-cee7-20f298c3b0c3"
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='MNIST Pipeline',\n",
    "   description='A toy pipeline that performs mnist model training and prediction.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def mnist_container_pipeline(\n",
    "    compile_optimizer: str = \"adam\",\n",
    "    epochs: int = 50,\n",
    "    validation_split: float = 0.15,\n",
    "):\n",
    "    # Create MNIST training component.\n",
    "    mnist_training_container = train_op(compile_optimizer, epochs, validation_split)\n",
    "    mnist_training_container.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "\n",
    "    # Create MNIST prediction component.\n",
    "    mnist_predict_container = test_op(mnist_training_container.outputs['data'])\n",
    "    \n",
    "    confusion_matrix_op(mnist_predict_container.outputs['labels_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compile pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1pqw3d0tnth"
   },
   "source": [
    "Finally we feed our pipeline definition into the compiler. We create a zip that can be uploaded as a pipeline in the kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "b1pqw3d0tnth"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'fashion_mnist_kubeflow'\n",
    "pipeline_func= mnist_container_pipeline\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Run pipeline\n",
    "Create a client to enable communication with the Pipelines API server. Please, provide your security token and your authservice_session to communicate with the pipeline from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOKIE ='YOUR_COOKIE'\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Disable ssl verification\n",
    "from kfp_server_api.configuration import Configuration as Configuration\n",
    "if 'old_init' not in globals():\n",
    "    old_init = Configuration.__init__\n",
    "\n",
    "def new_init(self, *k, **kw):\n",
    "    old_init(self, *k, **kw)\n",
    "    self.verify_ssl = False\n",
    "Configuration.__init__ = new_init\n",
    "cookies = COOKIE\n",
    "client = kfp.Client(host='http://istio-ingressgateway.istio-system.svc/pipeline', namespace='admin', cookies=cookies)\n",
    "client.list_experiments(namespace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cFLEOyq6Ep5",
    "outputId": "ac3009e4-4beb-4486-bc0a-ad30dfd2780f"
   },
   "source": [
    "Finally we run it as an experiment. This will give us 2 links at the bottom that we can follow to the Kubeflow Pipelines UI where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline.\n",
    "\n",
    "Define some environment variables which are to be used as inputs at various points in the pipeliner arguments, and run the experiment from the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cFLEOyq6Ep5",
    "outputId": "ac3009e4-4beb-4486-bc0a-ad30dfd2780f"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'fashion_mnist_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\n",
    "    \"compile_optimizer\" : \"adam\",\n",
    "    \"epochs\" : 1,\n",
    "    \"validation_split\" : 0.15,\n",
    "}\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name,\n",
    "                                                  namespace='admin',\n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KF Fashion MNIST",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
